<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=Content-Language content="en"><meta name=color-scheme content="light dark"><meta name=author content="Arjun Sehajpal"><meta name=description content="LLMs have been all the rage now. Whether you believe in the potential of LLMs or you are still in the camp of sceptics, it doesn’t matter. It’s the stakeholders’ opinion about LLMs that matters. This is because LLMs make it quite easy to “wow the users” and attract them to the product. So, they facilitate onboarding new users to the product. However, it is the trust that retains the users."><meta name=keywords content="blog,data science,machine learning,data engineering"><meta name=twitter:card content="summary"><meta name=twitter:title content="LLM Evaluation 01: Overview"><meta name=twitter:description content="LLMs have been all the rage now. Whether you believe in the potential of LLMs or you are still in the camp of sceptics, it doesn’t matter. It’s the stakeholders’ opinion about LLMs that matters. This is because LLMs make it quite easy to “wow the users” and attract them to the product. So, they facilitate onboarding new users to the product. However, it is the trust that retains the users."><meta property="og:title" content="LLM Evaluation 01: Overview"><meta property="og:description" content="LLMs have been all the rage now. Whether you believe in the potential of LLMs or you are still in the camp of sceptics, it doesn’t matter. It’s the stakeholders’ opinion about LLMs that matters. This is because LLMs make it quite easy to “wow the users” and attract them to the product. So, they facilitate onboarding new users to the product. However, it is the trust that retains the users."><meta property="og:type" content="article"><meta property="og:url" content="https://arjunsehajpal.com/posts/llm-evaluation-overview/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-11-08T00:38:16+05:30"><meta property="article:modified_time" content="2023-11-08T00:38:16+05:30"><title>LLM Evaluation 01: Overview · Arjun's Blog
</title><link rel=canonical href=https://arjunsehajpal.com/posts/llm-evaluation-overview/><link rel=preload href="https://arjunsehajpal.com/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=https://arjunsehajpal.com/css/coder.min.d9fddbffe6f27e69985dc5fe0471cdb0e57fbf4775714bc3d847accb08f4a1f6.css integrity="sha256-2f3b/+byfmmYXcX+BHHNsOV/v0d1cUvD2Eesywj0ofY=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=https://arjunsehajpal.com/img/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://arjunsehajpal.com/img/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=https://arjunsehajpal.com/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=https://arjunsehajpal.com/images/apple-touch-icon.png><meta name=generator content="Hugo 0.120.3"></head><body class="preload-transitions colorscheme-light"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://arjunsehajpal.com/>Arjun's Blog
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=https://arjunsehajpal.com/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=https://arjunsehajpal.com/about/>About</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://arjunsehajpal.com/posts/llm-evaluation-overview/>LLM Evaluation 01: Overview</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2023-11-08T00:38:16+05:30>November 8, 2023
</time></span><span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
4-minute read</span></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i>
<span class=tag><a href=https://arjunsehajpal.com/tags/llmops/>LLMOps</a></span></div></div></header><div><p>LLMs have been all the rage now. Whether you believe in the potential of LLMs or you are still in the camp of sceptics, it doesn’t matter. It’s the stakeholders’ opinion about LLMs that matters. This is because LLMs make it quite easy to “wow the users” and attract them to the product. So, they facilitate onboarding new users to the product. However, it is the trust that retains the users. And, as most of us know by now, LLMs make numerous mistakes. As the complexity rises and users receive incorrect answers frequently, they won’t trust the system and will revert to the tools they were using before.</p><p>This post is my attempt to consolidate the research around LLM evaluation and come up with a loose framework that enhances the trustworthiness of LLM-powered applications.</p><h2 id=1-why-traditional-evaluation-framework-wont-work>1. Why traditional evaluation framework won’t work?
<a class=heading-link href=#1-why-traditional-evaluation-framework-wont-work><i class="fa fa-link" aria-hidden=true></i></a></h2><p>When we developed Machine Learning models in the pre-LLM era (BC can now be Before ChatGPT), we had a precise checklist which formed our evaluation framework. We divided our input dataset into training, evaluation and test sets and measured the performance by calculating metrics on these datasets.</p><figure><img src=https://arjunsehajpal.com/blog-img/06-traditional-evaluation-framework.jpg><figcaption><h4>Fig 1: Traditional Evaluation Framework</h4></figcaption></figure><p>But this same approach doesn’t work for LLMs. This is because the traditional approach is based on two key pillars, i.e., data and metrics. As LLMs are expensive to train, most applications resort to calling an external API, provided by the OpenAIs and Anthropics of this world. And, good luck with getting the data for these models. But even if one has trained their own LLM (hence, they have data), the question that looks us in the eyes is “what metrics to calculate for evaluation?”. The output of LLMs are more on the qualitative side and are hard to quantify. Another challenge is General-Purpose nature of LLMs. They can perform a diverse set of behaviours. Thus, aggregate metrics become more or less inefficient.</p><p>To summarise, there are three challenges in evaluating the LLMs:</p><ul><li>Access to training data</li><li>Qualitative outputs, which are hard to quantify</li><li>Diversity of behaviour</li></ul><h2 id=2-makeshift-evaluation-framework-for-llms>2. Makeshift evaluation framework for LLMs
<a class=heading-link href=#2-makeshift-evaluation-framework-for-llms><i class="fa fa-link" aria-hidden=true></i></a></h2><p>As discussed in the previous section, traditional evaluation strategy needed a dataset and a bunch of metrics to calculate on that dataset. So, building on this, we will divide our approach into two steps:</p><ul><li>What dataset to use?</li><li>What metrics to measure?</li></ul><h3 id=21-evaluation-dataset>2.1. Evaluation dataset
<a class=heading-link href=#21-evaluation-dataset><i class="fa fa-link" aria-hidden=true></i></a></h3><p>A dataset is any set of records. As opposed to the supervised paradigm of model training, we don’t have input-output pairs to begin with. So, we will have to compile our own data. To do so, we can follow the following three-step strategy.</p><h4 id=211-look-out-for-interesting-cases>2.1.1. Look out for interesting cases
<a class=heading-link href=#211-look-out-for-interesting-cases><i class="fa fa-link" aria-hidden=true></i></a></h4><p>The best way to prepare the dataset is to just <strong>start</strong>. Accumulate the ad hoc prompts that you use. For instance, write a short paragraph about <code>subject</code>. Here, the subject could be anything, from an inanimate object to a pet.
As you are firing these prompts, there will be some unique and interesting cases where the LLM will fail to generate the “required” output and will require some tweaks in the prompt. You can consolidate these <em>tricky</em>, <em>hard</em> and <em>unique</em> examples into a small dataset.</p><h4 id=212-incrementally-add-more-data>2.1.2. Incrementally add more data
<a class=heading-link href=#212-incrementally-add-more-data><i class="fa fa-link" aria-hidden=true></i></a></h4><p>As the LLM application is rolled out, we can start capturing the real time feedback from the users. This feedback could be handled in two ways:</p><ul><li><em>Explicit feedback</em>: this sort of feedback is easier to capture. Here, we just ask the user whether he liked the answer generated or not. As OpenAI does, we can provide a thumbs-up and thumbs-down icon to collect this type of feedback.</li><li><em>Implicit feedback</em>: as opposed to explicit feedback, this feedback is relatively difficult to collect. One way to collect implicit feedback is to check if the user is asking the same question (different words but semantically having the similar meaning) multiple times.</li></ul><figure><img src=https://arjunsehajpal.com/blog-img/06-chatgpt-explicit-feedback.png><figcaption><h4>Fig 2: Explicit Feedback in ChatGPT</h4></figcaption></figure><p>Using explicit and implicit feedback, we can collect cases which users disliked the output for and add them to data created in the previous step. Also, we can look out for underrepresented topics, intents and documents (in other words, edge cases).</p></div><footer></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2023
Arjun Sehajpal
·
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script src=https://arjunsehajpal.com/js/coder.min.9cf2dbf9b6989ef8eae941ffb4231c26d1dc026bca38f1d19fdba50177d8a9ac.js integrity="sha256-nPLb+baYnvjq6UH/tCMcJtHcAmvKOPHRn9ulAXfYqaw="></script></body></html>