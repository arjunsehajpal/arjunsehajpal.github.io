<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=content-language content="en"><meta name=color-scheme content="light dark"><meta name=author content="Arjun Sehajpal"><meta name=description content="A detailed introduction to Model Calibration"><meta name=keywords content="blog,data science,machine learning,data engineering"><meta name=twitter:card content="summary"><meta name=twitter:title content="Monitoring the Machine Learning System"><meta name=twitter:description content="A detailed introduction to Model Calibration"><meta property="og:title" content="Monitoring the Machine Learning System"><meta property="og:description" content="A detailed introduction to Model Calibration"><meta property="og:type" content="article"><meta property="og:url" content="https://arjunsehajpal.com/posts/monitoring-the-ml-system/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-05-26T00:00:00+00:00"><meta property="article:modified_time" content="2023-05-26T00:00:00+00:00"><title>Monitoring the Machine Learning System · Arjun's Blog</title><link rel=canonical href=https://arjunsehajpal.com/posts/monitoring-the-ml-system/><link rel=preload href="https://arjunsehajpal.com/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=https://arjunsehajpal.com/css/coder.min.d9fddbffe6f27e69985dc5fe0471cdb0e57fbf4775714bc3d847accb08f4a1f6.css integrity="sha256-2f3b/+byfmmYXcX+BHHNsOV/v0d1cUvD2Eesywj0ofY=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=https://arjunsehajpal.com/img/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://arjunsehajpal.com/img/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=https://arjunsehajpal.com/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=https://arjunsehajpal.com/images/apple-touch-icon.png><meta name=generator content="Hugo 0.112.3"></head><body class="preload-transitions colorscheme-light"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://arjunsehajpal.com/>Arjun's Blog</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=https://arjunsehajpal.com/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=https://arjunsehajpal.com/about/>About</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://arjunsehajpal.com/posts/monitoring-the-ml-system/>Monitoring the Machine Learning System</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2023-05-26T00:00:00Z>May 26, 2023</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
5-minute read</span></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i>
<span class=tag><a href=https://arjunsehajpal.com/tags/mlops/>MLOPs</a></span></div></div></header><div><p>Almost every Machine Learning pipeline starts from the raw data and ends at decisions. Any amount of bad data can lead to faulty decisions, which could further lead to loss of business (and in some cases, organisation’s reputation). As the data pipelines grow more and more complex, it becomes increasingly important to have a robust monitoring and observability structure in place. Neglecting this component results in monitoring debt, which is paid back in Developer’s time.</p><h1 id=monitoring-debt>Monitoring Debt
<a class=heading-link href=#monitoring-debt><i class="fa fa-link" aria-hidden=true></i></a></h1><p>Analogous to technical debt, monitoring debt represents the gap between the monitoring ideals and monitoring reality. Much like technical debt, where we factor how the cost of redesigning and refactoring tomorrow allows us to ship the code today, in monitoring debt, a team gives up the ability to catch issues ahead well ahead of the stakeholders to release fast. But not enough time is not the only issue that the code becomes indebted, it can also be due to incompetence.</p><p>Like financial debt, monitoring debt can be a good thing. But repaying monitoring debt (or technical debt of any kind) is difficult. And, it usually boils down to two scenarios:</p><ol><li>Will the same developers be around to pay the debt?</li><li>If they are not, does the new developer have the documentation to acquire the intimate knowledge of the codebase? This must include:<ul><li>What behaviour is normal?</li><li>What are high-priority events one should monitor for?</li></ul></li></ol><p>It is best to have <strong>yes</strong> as an answer to the first scenario, because if the old developers didn’t have time to put proper monitoring and logging in place, you can’t expect much from documentation either.</p><p>If one doesn’t want to end up in these scenarios, it is best to embrace the three truths about monitoring.</p><ol><li><em>Monitoring is high-skilled activity</em> - As stated above, to enable monitoring in a system requires one to have clear knowledge of monitoring goals and monitoring mechanism. The former requires in-depth knowledge of the functioning of the system to be monitored, while the latter requires knowledge of how to instrument the system.</li><li><em>Monitoring is best done fresh</em> - Retrofitting logging statements into your code doesn’t work well as it could lead to compatibility issues. These compatibility issues might demand code changes as well. Hence, the longer a system goes without monitoring, the harder it becomes to introduce monitoring.</li><li><em>ML monitoring doesn’t equal traditional monitoring</em> - Traditional monitoring techniques don&rsquo;t work in Machine Learning pipelines interacting with data, which is not under your control.</li></ol><h1 id=embracing-the-monitoring>Embracing the Monitoring
<a class=heading-link href=#embracing-the-monitoring><i class="fa fa-link" aria-hidden=true></i></a></h1><p>Monitoring a machine learning pipeline often boils down to tying together the disparate elements of quality analysis. These quality analyses shouldn&rsquo;t just stop at data. A holistic monitoring system should run quality checks on data, but shouldn&rsquo;t just limit itself to it. It should also monitor the health of pipelines and drift the model is experiencing. Hence, a monitoring pipeline should have three components:</p><ol><li>Data Quality Monitoring</li><li>Pipeline Health Monitoring</li><li>Model Drift Monitoring</li></ol><h2 id=data-quality-monitoring>Data Quality Monitoring
<a class=heading-link href=#data-quality-monitoring><i class="fa fa-link" aria-hidden=true></i></a></h2><p>The term “data quality” is context dependent. Whether a data is a quality dataset depends upon the purpose it is being served for. Data that captures firmographic details of all customers might be good for Dashboards, but might not be good for training a model. Similarly, a dataset capturing embeddings might be of no use to Business Analysts working on PowerBI, despite being a goldmine for Data Scientist. So, in this situation, it is really weird to stamp some arbitrary percentage number onto the data, communicating how much “quality” it carries. For instance, a 97% quality score for data means nothing if it is not serving the purpose it intends to.</p><p>Rather than talking about something abstract like scores and percentages, it is better to talk about objectives. These objectives are the key things that we want to measure when monitoring data health, and they could be use-case dependent. These objectives could define what is “good-enough” data for us. An example of objectives could be as follow (here, I am mapping the objective I want to achieve with the <a href=https://greatexpectations.io/expectations/>GreatExpectation</a>’s expectation):</p><table><thead><tr><th>Objective</th><th>Identifier</th><th>Threshold</th></tr></thead><tbody><tr><td><code>CustomerID</code> should be unique</td><td><code>expect_column_values_to_be_unique</code></td><td>NA</td></tr><tr><td>Column <code>MobileOS</code> should be one of Android or iOS</td><td><code>expect_column_values_to_be_in_set</code></td><td>NA</td></tr><tr><td>Column <code>Age</code> should have values between 18 and 100</td><td><code>expect_column_values_to_be_between</code></td><td>0.99</td></tr></tbody></table><p>As evident from the table, a quality data for me would be the one where CustomerID is unique and MobileOS just contains two mentioned values. For the Age column, I am expecting the values to be between 18 and 100, but I will accept the 1% values that don’t satisfy this assumption. To elaborate:</p><ul><li><em>Objective</em> is the quality metric we are tracking</li><li><em>Identifier</em> is how we measure this metric. Here, we are using <code>GreatExpectation</code> as our identifier mechanism.</li><li><em>Threshold</em> is the wiggle room we are comfortable with giving away.</li></ul><p>Jotting down our Objectives/Identifier provide us with significant benefits:</p><ol><li>The objectives/identifiers defined above can be easily translated into JSON or YAML and fed into the monitoring logic. Thus, decoupling the execution engine and test we want to perform.</li><li>Though the tests can be performed at the same time, different teams can derive the definition of “good-enough” for their use-case by cherry-picking the results of the objectives that are of interest to them. For instance, if someone is only interested in <code>CustomerID</code> and <code>Age</code>, they can ignore the result of the second objective, as it is of no use to them.</li></ol></div><footer></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2023
Arjun Sehajpal
·
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script src=https://arjunsehajpal.com/js/coder.min.9cf2dbf9b6989ef8eae941ffb4231c26d1dc026bca38f1d19fdba50177d8a9ac.js integrity="sha256-nPLb+baYnvjq6UH/tCMcJtHcAmvKOPHRn9ulAXfYqaw="></script></body></html>