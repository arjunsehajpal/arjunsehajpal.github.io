<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Arjun&#39;s Blog</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Arjun&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sun, 23 Nov 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>About</title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Sun, 23 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/about/</guid>
      <description>&lt;p&gt;Hi! I am Arjun Sehajpal. I ply my trade as a Staff Machine Learning Engineer at Visa, where I’m building a centralized AI platform that integrates open-source monitoring tools and governance APIs, enabling automated fairness, explainability, and model catalogue workflows.&lt;/br&gt;&lt;/p&gt;&#xA;&lt;p&gt;Before this, I spent almost five years at &lt;a href=&#34;https://www.zs.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ZS&lt;/a&gt; across roles in Data Science and AI Engineering. My work there focused on designing and engineering AI/LLM systems, which included developing internal ML libraries, improving reproducibility and experimentation, and personalization systems.&lt;/br&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM Evaluation 02: RAG</title>
      <link>http://localhost:1313/posts/llm-evaluation-02/</link>
      <pubDate>Mon, 08 Jul 2024 00:38:16 +0530</pubDate>
      <guid>http://localhost:1313/posts/llm-evaluation-02/</guid>
      <description>&lt;p&gt;Once perceived as a hack, Retrieval-Augmented Generation (RAG) has now become an essential component of LLM applications. It has been most impactful in the area of closed-domain question answering, which was considered tricky due to static knowledge bases and limited context understanding. Since RAG systems retrieve information in real-time from a continuously updated corpus (thanks to vector databases), there is no need for manual updates to a static knowledge base. By combining retrieval and generation, RAG-enabled LLM systems can better understand the context and nuances of complex queries, leading to more accurate and relevant answers. The effectiveness of RAG-enabled LLM systems has been so significant that entire ecosystems of startups and vector database providers have emerged around them.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM Evaluation 01: Overview</title>
      <link>http://localhost:1313/posts/llm-evaluation-overview/</link>
      <pubDate>Mon, 18 Mar 2024 00:38:16 +0530</pubDate>
      <guid>http://localhost:1313/posts/llm-evaluation-overview/</guid>
      <description>&lt;p&gt;LLMs have been all the rage now. Whether you believe in the potential of LLMs or you are still in the camp of sceptics, it doesn’t matter. It’s the stakeholders’ opinion about LLMs that matters. This is because LLMs make it quite easy to “wow the users” and attract them to the product. So, they facilitate onboarding new users to the product. However, it is the trust that retains the users. And, as most of us know by now, LLMs make numerous mistakes. As the complexity rises and users receive incorrect answers frequently, they won’t trust the system and will revert to the tools they were using before.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Monitoring the Machine Learning System</title>
      <link>http://localhost:1313/posts/monitoring-the-ml-system/</link>
      <pubDate>Fri, 26 May 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/monitoring-the-ml-system/</guid>
      <description>&lt;p&gt;Almost every Machine Learning pipeline starts from the raw data and ends at decisions. Any amount of bad data can lead to faulty decisions, which could further lead to loss of business (and in some cases, organisation’s reputation). As the data pipelines grow more and more complex, it becomes increasingly important to have a robust monitoring and observability structure in place. Neglecting this component results in monitoring debt, which is paid back in Developer’s time.&lt;/p&gt;</description>
    </item>
    <item>
      <title>On Conway’s Law</title>
      <link>http://localhost:1313/posts/conways-law/</link>
      <pubDate>Sun, 11 Dec 2022 00:38:16 +0530</pubDate>
      <guid>http://localhost:1313/posts/conways-law/</guid>
      <description>&lt;p&gt;Software design is an iterative process. Its longevity depends upon thousands of small design decisions taken every day, and each decision should be in sync with the other. This constant attention to detail is what makes a software system tick. Hence, the quality of software depends hugely on the efficiency of communication between humans building it. This observation became the basis of Conway’s Law.&lt;/p&gt;&#xA;&lt;p&gt;Paraphrasing Dr. Melvin E. Conway, “Any organization that designs a system, will produce a design whose structure is a copy of the organization’s communication structure.” Though Dr. Conway used the term “system” in context to System Theory, where everything is a system and everything is a part of a system. And it is certainly true for software systems. Conway’s law is an observational law, which tries to build a connection between innovation and the impact and working methodologies and structure of the organization. A classic example of this law would be Apple’s iOS and Android, with the former being a closed in-house project and the latter being developed by open-source contributors all over the globe. This difference is quite obvious as iOS despite being very efficient, doesn’t gel well with others. On the other hand, Android is more flexible but isn’t as well organized as its counterpart.&lt;/p&gt;</description>
    </item>
    <item>
      <title>NFRs for Data Science Products</title>
      <link>http://localhost:1313/posts/data-science-nfrs/</link>
      <pubDate>Sat, 04 Jun 2022 01:25:27 +0530</pubDate>
      <guid>http://localhost:1313/posts/data-science-nfrs/</guid>
      <description>&lt;p&gt;Non-functional requirements (NFRs) are as important as functional requirements. While the system can still work if the NFRs are not met, it may not meet the stakeholders’ expectations. A functional requirement (FR) defines the specific behaviour of a system, whereas an NFR is a criterion to judge the operation of the system. For instance, a system should show a “list of orders” by the user is defined as a functional requirement, and how much time it takes for a page to load and show the first 10 orders is defined as a non-functional requirement. Hence, NFRs refer to the general qualities that provide a good user experience.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Model Calibration: Intuition and Implementation</title>
      <link>http://localhost:1313/posts/model_calibration/</link>
      <pubDate>Wed, 09 Feb 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/model_calibration/</guid>
      <description>&lt;p&gt;Depending upon the nature of the problem and the functioning of the model, the format in which output is provided can differ from model to model. For instance, in a multiclass classification problem, a decision tree provides a label, which is narrowed down going down the tree, where each node reduces the set of possible outcomes to one. On the contrary, the neural networks provide a vector of continuous values calculated by applying non-linear transformations to the input vector. In the latter case, we have to transform the output vector to a single label (here, looking at you, argmax).  Here, there might be a temptation to use these values as the confidence of the model in each label class. After all, isn’t the model that has twice the confidence in the label class for which output is 0.6 to the label class for which the output is 0.3? NO. Not until the model is calibrated.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why evaluating forecasts is hard?</title>
      <link>http://localhost:1313/posts/forecasting-brier-score/</link>
      <pubDate>Wed, 12 Jan 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/forecasting-brier-score/</guid>
      <description>&lt;p&gt;Judging forecasts can be insanely hard than they are supposed to be. As time passes, the accuracy of a particular forecast seems obvious, but it is hardly so. The crux of the problem is the property that makes the forecasting different from classical predictions, i.e., time. The presence of temporal components makes it difficult to implement the techniques that we would have otherwise used, for instance, randomized control experiments. When it comes to time-series problems, only an omniscient being can replay a point in time multiple times to judge the effectiveness of the forecast. But unfortunately (or fortunately), we are no God, and unlike Dr. Strange, we got no &lt;em&gt;Eye of Agamotto&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Contact</title>
      <link>http://localhost:1313/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/contact/</guid>
      <description></description>
    </item>
    <item>
      <title>Projects</title>
      <link>http://localhost:1313/projects/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/</guid>
      <description>&lt;p&gt;Nothing to see here&amp;hellip; Move along!&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
