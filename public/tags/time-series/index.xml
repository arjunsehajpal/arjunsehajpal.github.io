<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Time-Series on Arjun&#39;s Blog</title>
    <link>http://localhost:1313/tags/time-series/</link>
    <description>Recent content in Time-Series on Arjun&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Wed, 12 Jan 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/time-series/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Why evaluating forecasts is hard?</title>
      <link>http://localhost:1313/posts/forecasting-brier-score/</link>
      <pubDate>Wed, 12 Jan 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/forecasting-brier-score/</guid>
      <description>&lt;p&gt;Judging forecasts can be insanely hard than they are supposed to be. As time passes, the accuracy of a particular forecast seems obvious, but it is hardly so. The crux of the problem is the property that makes the forecasting different from classical predictions, i.e., time. The presence of temporal components makes it difficult to implement the techniques that we would have otherwise used, for instance, randomized control experiments. When it comes to time-series problems, only an omniscient being can replay a point in time multiple times to judge the effectiveness of the forecast. But unfortunately (or fortunately), we are no God, and unlike Dr. Strange, we got no &lt;em&gt;Eye of Agamotto&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
