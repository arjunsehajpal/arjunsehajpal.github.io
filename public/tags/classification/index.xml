<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Classification on Arjun&#39;s Blog</title>
    <link>http://localhost:1313/tags/classification/</link>
    <description>Recent content in Classification on Arjun&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Wed, 09 Feb 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/classification/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Model Calibration: Intuition and Implementation</title>
      <link>http://localhost:1313/posts/model_calibration/</link>
      <pubDate>Wed, 09 Feb 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/model_calibration/</guid>
      <description>&lt;p&gt;Depending upon the nature of the problem and the functioning of the model, the format in which output is provided can differ from model to model. For instance, in a multiclass classification problem, a decision tree provides a label, which is narrowed down going down the tree, where each node reduces the set of possible outcomes to one. On the contrary, the neural networks provide a vector of continuous values calculated by applying non-linear transformations to the input vector. In the latter case, we have to transform the output vector to a single label (here, looking at you, argmax).  Here, there might be a temptation to use these values as the confidence of the model in each label class. After all, isnâ€™t the model that has twice the confidence in the label class for which output is 0.6 to the label class for which the output is 0.3? NO. Not until the model is calibrated.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
